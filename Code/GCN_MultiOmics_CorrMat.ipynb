{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCS261zhs9DN","executionInfo":{"status":"ok","timestamp":1741371375920,"user_tz":420,"elapsed":830,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"8d7c3d88-7bb4-48c3-b237-d62ba70e48eb"},"id":"cCS261zhs9DN","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install -q torch_geometric"],"metadata":{"id":"4eIydkWXs-Ri","executionInfo":{"status":"ok","timestamp":1741371377992,"user_tz":420,"elapsed":2074,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"id":"4eIydkWXs-Ri","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# GCN using multi-omics data (mRNA, miRNA and DNA methylation) with correlation matrix graph structure (5 fold cross validation)\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GraphConv\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n","from torch.nn import BatchNorm1d, LeakyReLU\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import datetime\n","now = datetime.datetime.now\n","\n","# Check if GPU is available and set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"],"metadata":{"id":"lnQaPpr_tE57","executionInfo":{"status":"ok","timestamp":1741371382713,"user_tz":420,"elapsed":4718,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b1d4ce7-4275-4eef-ee5a-043b128ebd8e"},"id":"lnQaPpr_tE57","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# Step 1: Load the PPI data\n","ppi_file_path = 'drive/My Drive/Projects/Gene_Expression_Project/PPI.csv'\n","ppi_df = pd.read_csv(ppi_file_path)\n","\n","# Step 2: Concatenate 'stringId_A' and 'stringId_B' to calculate the number of connections (degree)\n","all_proteins = pd.concat([ppi_df['stringId_A'], ppi_df['stringId_B']])\n","\n","# Step 3: Count the number of connections for each protein\n","protein_connections = all_proteins.value_counts()\n","\n","# Step 4: Define a degree threshold to select only highly connected proteins (e.g., 5 or more connections)\n","degree_threshold = 200\n","high_degree_proteins = protein_connections[protein_connections >= degree_threshold].index\n","\n","# Step 5: Filter the PPI data to include only edges where both proteins have a high number of connections\n","ppi_filtered = ppi_df[ppi_df['stringId_A'].isin(high_degree_proteins) & ppi_df['stringId_B'].isin(high_degree_proteins)]\n","\n","# Step 6: Map the high-degree proteins to unique node IDs\n","proteins = pd.concat([ppi_filtered['stringId_A'], ppi_filtered['stringId_B']]).unique()\n","protein_to_id = {protein: idx for idx, protein in enumerate(proteins)}\n","\n","# Step 7: Create edge index (this will be the input for GCN)\n","edges = ppi_filtered[['stringId_A', 'stringId_B']].map(lambda x: protein_to_id[x])\n","edge_index = torch.tensor(edges.values.T, dtype=torch.long)"],"metadata":{"id":"ykm0t0OstE-3","executionInfo":{"status":"ok","timestamp":1741371382761,"user_tz":420,"elapsed":23,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"id":"ykm0t0OstE-3","execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Step 8: Load and preprocess the multi-omics data\n","!wget https://www.webpages.uidaho.edu/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n","file_path = 'mRNA_miRNA_Meth_integrated.csv'\n","df = pd.read_csv(file_path)\n","df.drop(df.columns[0], axis=1, inplace=True)\n","Y = df.iloc[:, -1].copy()\n","\n","# Remove non-numeric columns\n","df = df.select_dtypes(include=[np.number])\n","X = df.values\n","\n","num_classes = len(set(Y))\n","print(\"Number of classes:\", num_classes)\n","num_samples = X.shape[0]\n","print(\"Number of samples:\", num_samples)\n","num_Features = X.shape[1]\n","print(\"Number of Features:\", num_Features)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Encode labels\n","label_encoder = LabelEncoder()\n","Y = label_encoder.fit_transform(Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQsfLKyjtfLf","executionInfo":{"status":"ok","timestamp":1741371394147,"user_tz":420,"elapsed":11370,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"7a14f44a-9332-4704-baee-55e465c6b78b"},"id":"sQsfLKyjtfLf","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-07 18:15:43--  https://www.webpages.uidaho.edu/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n","Resolving www.webpages.uidaho.edu (www.webpages.uidaho.edu)... 129.101.105.230\n","Connecting to www.webpages.uidaho.edu (www.webpages.uidaho.edu)|129.101.105.230|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 123599052 (118M) [application/octet-stream]\n","Saving to: ‘mRNA_miRNA_Meth_integrated.csv.1’\n","\n","mRNA_miRNA_Meth_int 100%[===================>] 117.87M  22.9MB/s    in 7.5s    \n","\n","2025-03-07 18:15:51 (15.7 MB/s) - ‘mRNA_miRNA_Meth_integrated.csv.1’ saved [123599052/123599052]\n","\n","Number of classes: 32\n","Number of samples: 8464\n","Number of Features: 2793\n"]}]},{"cell_type":"code","source":["# Step 9: Define the GCN model\n","class GCN(nn.Module):\n","    def __init__(self, in_feats, hidden_feats, num_classes, num_layers=3, dropout=0.3):  # Applied num_layers=3, dropout=0.3\n","        super(GCN, self).__init__()\n","        self.conv_layers = nn.ModuleList([GraphConv(in_feats, hidden_feats)])\n","        self.conv_layers.extend([GraphConv(hidden_feats, hidden_feats) for _ in range(num_layers - 1)])\n","        self.final_conv = GraphConv(hidden_feats, num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        for conv in self.conv_layers:\n","            x = torch.relu(conv(x, edge_index))\n","            x = self.dropout(x)\n","        x = self.final_conv(x, edge_index)\n","        return x\n","\n","# Step 10: Set up K-fold cross-validation\n","k = 5\n","kf = KFold(n_splits=k, shuffle=True)\n","\n","# Initialize lists to store metrics for each fold\n","precision_scores = []\n","recall_scores = []\n","accuracy_scores = []\n","F1Measure = []"],"metadata":{"id":"lVkRGkKeuj4S","executionInfo":{"status":"ok","timestamp":1741371394152,"user_tz":420,"elapsed":3,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"id":"lVkRGkKeuj4S","execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Step 11: Training and Evaluation\n","t = now()\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = Y[train_index], Y[test_index]\n","\n","    X_train = torch.FloatTensor(X_train).to(device)\n","    y_train = torch.LongTensor(y_train).to(device)\n","    X_test = torch.FloatTensor(X_test).to(device)\n","    y_test = torch.LongTensor(y_test).to(device)\n","\n","    # Calculate the correlation matrix and convert it to an edge index\n","    correlation_matrix_train = np.corrcoef(X_train.cpu(), rowvar=True)\n","    correlation_matrix_test = np.corrcoef(X_test.cpu(), rowvar=True)\n","\n","    # Create edge indices based on a correlation threshold\n","    src_train, dst_train = np.where(correlation_matrix_train > 0.9)\n","    src_test, dst_test = np.where(correlation_matrix_test > 0.9)\n","\n","    edge_index_train = torch.tensor([src_train, dst_train], dtype=torch.long).to(device)\n","    edge_index_test = torch.tensor([src_test, dst_test], dtype=torch.long).to(device)\n","\n","    # Create PyTorch Geometric data objects\n","    train_data = Data(x=X_train, edge_index=edge_index_train).to(device)\n","    test_data = Data(x=X_test, edge_index=edge_index_test).to(device)\n","\n","    # Define the model, loss function, and optimizer\n","    input_feats = X.shape[1]\n","    hidden_feats = 512  # Applied hidden_feats=512\n","    num_classes = len(np.unique(Y))\n","    model = GCN(input_feats, hidden_feats, num_classes).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Applied learning_rate=0.001\n","\n","    # Training loop\n","    num_epochs = 100\n","    for epoch in range(num_epochs):\n","        model.train()\n","        outputs = model(train_data)\n","        loss = criterion(outputs, y_train)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            y_pred = torch.argmax(model(test_data), dim=1)\n","            acc = accuracy_score(y_test.cpu().numpy(), y_pred.cpu().numpy())\n","            # print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        y_pred = torch.argmax(model(test_data), dim=1)\n","        test_acc = accuracy_score(y_test.cpu().numpy(), y_pred.cpu().numpy())\n","        precision = precision_score(y_test.cpu().numpy(), y_pred.cpu().numpy(), average='macro', zero_division=1)\n","        recall = recall_score(y_test.cpu().numpy(), y_pred.cpu().numpy(), average='macro')\n","        f1 = f1_score(y_test.cpu().numpy(), y_pred.cpu().numpy(), average='macro')\n","\n","        accuracy_scores.append(test_acc)\n","        precision_scores.append(precision)\n","        recall_scores.append(recall)\n","        F1Measure.append(f1)\n","\n","print('Training time: %s' % (now() - t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTWakKA4uzeW","executionInfo":{"status":"ok","timestamp":1741371426521,"user_tz":420,"elapsed":32363,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"e97cfff7-ed72-4940-b15d-c812a11ad035"},"id":"QTWakKA4uzeW","execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-71dec2c51f3e>:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  edge_index_train = torch.tensor([src_train, dst_train], dtype=torch.long).to(device)\n"]},{"output_type":"stream","name":"stdout","text":["Training time: 0:00:32.332915\n"]}]},{"cell_type":"code","source":["# Calculate the average metrics across all folds\n","average_accuracy = np.mean(accuracy_scores)\n","average_precision = np.mean(precision_scores)\n","average_recall = np.mean(recall_scores)\n","average_f1 = np.mean(F1Measure)\n","\n","print(\"Average accuracy =\", average_accuracy)\n","print(\"Accuracy std dev =\", np.std(accuracy_scores))\n","print(\"Average precision =\", average_precision)\n","print(\"Precision std dev =\", np.std(precision_scores))\n","print(\"Average recall =\", average_recall)\n","print(\"Recall std dev =\", np.std(recall_scores))\n","print(\"Average F1 score =\", average_f1)\n","print(\"F1 std dev =\", np.std(F1Measure))"],"metadata":{"id":"9eENGPiEtp1s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741371426538,"user_tz":420,"elapsed":10,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"da0085b8-9460-4ad6-b032-ed42ebc60683"},"id":"9eENGPiEtp1s","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy = 0.954749008223264\n","Accuracy std dev = 0.006867695969203494\n","Average precision = 0.9427467270463765\n","Precision std dev = 0.010961365902894782\n","Average recall = 0.9269394591885003\n","Recall std dev = 0.014387885857901074\n","Average F1 score = 0.929841542011934\n","F1 std dev = 0.012093438190116951\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
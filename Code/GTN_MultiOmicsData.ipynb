{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTN Using multiOmics(mRNA, miRNA and DNA methylation) Data with PPI Network(5 fold Cross validation)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Load the PPI data\n",
    "ppi_file_path = 'Link to PPI'\n",
    "ppi_df = pd.read_csv(ppi_file_path)\n",
    "\n",
    "# Step 2: Concatenate 'stringId_A' and 'stringId_B' to calculate the number of connections (degree)\n",
    "all_proteins = pd.concat([ppi_df['stringId_A'], ppi_df['stringId_B']])\n",
    "\n",
    "# Step 3: Count the number of connections for each protein\n",
    "protein_connections = all_proteins.value_counts()\n",
    "\n",
    "# Step 4: Define a degree threshold to select only highly connected proteins (e.g., 200 or more connections)\n",
    "degree_threshold = 200\n",
    "high_degree_proteins = protein_connections[protein_connections >= degree_threshold].index\n",
    "\n",
    "# Step 5: Filter the PPI data to include only edges where both proteins have a high number of connections\n",
    "ppi_filtered = ppi_df[\n",
    "    ppi_df['stringId_A'].isin(high_degree_proteins) &\n",
    "    ppi_df['stringId_B'].isin(high_degree_proteins)\n",
    "]\n",
    "\n",
    "# Step 6: Map the high-degree proteins to unique node IDs\n",
    "proteins = pd.concat([ppi_filtered['stringId_A'], ppi_filtered['stringId_B']]).unique()\n",
    "protein_to_id = {protein: idx for idx, protein in enumerate(proteins)}\n",
    "\n",
    "# Step 7: Create edge index (this will be the input for GTN)\n",
    "edges = ppi_filtered[['stringId_A', 'stringId_B']].applymap(lambda x: protein_to_id[x])\n",
    "edge_index = torch.tensor(edges.values.T, dtype=torch.long).to(device)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# Step 8: Load and preprocess the multi-omics data\n",
    "file_path = 'Link to Dataset'\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "Y = df.iloc[:, -1].copy()\n",
    "\n",
    "# Remove non-numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "X = df.values\n",
    "\n",
    "num_classes = len(set(Y))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "num_samples = X.shape[0]\n",
    "print(\"Number of samples:\", num_samples)\n",
    "num_Features = X.shape[1]\n",
    "print(\"Number of Features:\", num_Features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Convert data to PyTorch tensors and move to device\n",
    "X = torch.tensor(X, dtype=torch.float).to(device)\n",
    "Y = torch.tensor(Y, dtype=torch.long).to(device)\n",
    "\n",
    "# Step 9: Create PyTorch Geometric data object using the edge_index from the filtered PPI network\n",
    "data = Data(x=X, edge_index=edge_index)\n",
    "\n",
    "# Step 10: Define the GTN model with best hyperparameters\n",
    "class GTN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_feats=1024, num_layers=2, dropout=0.5):\n",
    "        super(GTN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(TransformerConv(num_features, hidden_feats, heads=1, dropout=dropout))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(TransformerConv(hidden_feats, hidden_feats, heads=1, dropout=dropout))\n",
    "        self.fc = nn.Linear(hidden_feats, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 11: Set up K-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "F1Measure = []\n",
    "\n",
    "# Set hyperparameters\n",
    "hidden_feats = 1024\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "lr = 0.001\n",
    "weight_decay = 0\n",
    "num_epochs = 100\n",
    "\n",
    "# Step 12: Training and Evaluation\n",
    "for train_index, test_index in kf.split(X.cpu()):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Create train/test data using the same PPI edge_index\n",
    "    train_data = Data(x=X_train, edge_index=edge_index)\n",
    "    test_data = Data(x=X_test, edge_index=edge_index)\n",
    "\n",
    "    # Create the GTN model\n",
    "    model = GTN(\n",
    "        num_features=X.shape[1],\n",
    "        num_classes=len(set(Y.cpu().numpy())),\n",
    "        hidden_feats=hidden_feats,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(test_data)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n",
    "            scheduler.step(acc)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_data)\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        test_acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "\n",
    "        accuracy_scores.append(test_acc)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        F1Measure.append(f1)\n",
    "\n",
    "# Calculate the average metrics across all folds\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_precision = np.mean(precision_scores)\n",
    "average_recall = np.mean(recall_scores)\n",
    "average_f1 = np.mean(F1Measure)\n",
    "\n",
    "print(\"Average accuracy =\", average_accuracy)\n",
    "print(\"Accuracy Std Dev =\", np.std(accuracy_scores))\n",
    "print(\"Average precision =\", average_precision)\n",
    "print(\"Precision Std Dev =\", np.std(precision_scores))\n",
    "print(\"Average recall =\", average_recall)\n",
    "print(\"Recall Std Dev =\", np.std(recall_scores))\n",
    "print(\"Average F1 Measure =\", average_f1)\n",
    "print(\"F1 Std Dev =\", np.std(F1Measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d166c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GTN Using multiOmics(mRNA, miRNA and DNA methylation) Data with correlation matrix(5 fold Cross validation)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using:\", device)\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'Link to Dataset'\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "Y = df.iloc[:, -1].copy()\n",
    "\n",
    "# Remove non-numeric columns\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "X = df.values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "num_classes = len(set(Y))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "num_samples = X.shape[0]\n",
    "print(\"Number of samples:\", num_samples)\n",
    "num_Features = X.shape[1]\n",
    "print(\"Number of Features:\", num_Features)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Define the GTN model\n",
    "class GTN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GTN, self).__init__()\n",
    "        self.conv1 = TransformerConv(num_features, 128, heads=8)  # Output: 128 * 8 = 1024\n",
    "        self.conv2 = TransformerConv(1024, num_classes, heads=1)  # Input: 1024, Output: num_classes * 1\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Set up K-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "F1Measure = []\n",
    "\n",
    "# Cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    # Convert labels to tensors within the loop\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    # Calculate the correlation matrix and convert it to an edge index\n",
    "    correlation_matrix_train = np.corrcoef(X_train, rowvar=True)\n",
    "    correlation_matrix_test = np.corrcoef(X_test, rowvar=True)\n",
    "\n",
    "    # Create edge indices based on a correlation threshold\n",
    "    edge_index_train = torch.tensor(np.argwhere((correlation_matrix_train >= 0.9) | (correlation_matrix_train <= -0.9)).T, dtype=torch.long).to(device)\n",
    "    edge_index_test = torch.tensor(np.argwhere((correlation_matrix_test >= 0.9) | (correlation_matrix_test <= -0.9)).T, dtype=torch.long).to(device)\n",
    "\n",
    "    # Prepare training and testing data\n",
    "    train_data = Data(x=torch.tensor(X_train, dtype=torch.float32).to(device), edge_index=edge_index_train, y=y_train)\n",
    "    test_data = Data(x=torch.tensor(X_test, dtype=torch.float32).to(device), edge_index=edge_index_test, y=y_test)\n",
    "\n",
    "    # Initialize the model, criterion, optimizer, and scheduler\n",
    "    model = GTN(X.shape[1], len(np.unique(labels))).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjusted learning rate\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        loss = criterion(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(test_data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
    "        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
    "\n",
    "        accuracy_scores.append(acc)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        F1Measure.append(f1)\n",
    "\n",
    "# Calculate the average metrics across all folds\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_precision = np.mean(precision_scores)\n",
    "average_recall = np.mean(recall_scores)\n",
    "average_f1 = np.mean(F1Measure)\n",
    "\n",
    "print(\"Average accuracy =\", average_accuracy)\n",
    "print(\"Accuracy Std Dev =\", np.std(accuracy_scores))\n",
    "print(\"Average precision =\", average_precision)\n",
    "print(\"Precision Std Dev =\", np.std(precision_scores))\n",
    "print(\"Average recall =\", average_recall)\n",
    "print(\"Recall Std Dev =\", np.std(recall_scores))\n",
    "print(\"Average F1 Measure =\", average_f1)\n",
    "print(\"F1 Std Dev =\", np.std(F1Measure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36uvxgtPcRlS","executionInfo":{"status":"ok","timestamp":1741374488589,"user_tz":420,"elapsed":19353,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"e5896649-2fc7-4c88-b38e-dd37ae3d36f4"},"id":"36uvxgtPcRlS","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -q torch_geometric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKxbDYxUcUYy","executionInfo":{"status":"ok","timestamp":1741374491760,"user_tz":420,"elapsed":3169,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"5390ac1b-5483-4026-903d-c599c1877498"},"id":"RKxbDYxUcUYy","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# GTN using multi-omics data (mRNA, miRNA and DNA methylation) with correlation matrix graph structure (5 fold cross validation)\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch_geometric.data import Data\n","from torch_geometric.nn import TransformerConv\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import datetime\n","now = datetime.datetime.now\n","\n","# Check if GPU is available and set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qMf4PWEciul","executionInfo":{"status":"ok","timestamp":1741374505269,"user_tz":420,"elapsed":13507,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"ec9c28f5-4172-48a7-980d-c2763fd7b58b"},"id":"7qMf4PWEciul","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["# Step 1: Load the PPI data\n","ppi_file_path = 'drive/My Drive/Projects/Gene_Expression_Project/PPI.csv'\n","ppi_df = pd.read_csv(ppi_file_path)"],"metadata":{"id":"ophEWdSgco7t","executionInfo":{"status":"ok","timestamp":1741374505955,"user_tz":420,"elapsed":680,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"id":"ophEWdSgco7t","execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Step 2: Concatenate 'stringId_A' and 'stringId_B' to calculate the number of connections (degree)\n","all_proteins = pd.concat([ppi_df['stringId_A'], ppi_df['stringId_B']])\n","\n","# Step 3: Count the number of connections for each protein\n","protein_connections = all_proteins.value_counts()\n","\n","# Step 4: Define a degree threshold to select only highly connected proteins (e.g., 200 or more connections)\n","degree_threshold = 200\n","high_degree_proteins = protein_connections[protein_connections >= degree_threshold].index\n","\n","# Step 5: Filter the PPI data to include only edges where both proteins have a high number of connections\n","ppi_filtered = ppi_df[\n","    ppi_df['stringId_A'].isin(high_degree_proteins) &\n","    ppi_df['stringId_B'].isin(high_degree_proteins)\n","]\n","\n","# Step 6: Map the high-degree proteins to unique node IDs\n","proteins = pd.concat([ppi_filtered['stringId_A'], ppi_filtered['stringId_B']]).unique()\n","protein_to_id = {protein: idx for idx, protein in enumerate(proteins)}\n","\n","# Step 7: Create edge index (this will be the input for GTN)\n","edges = ppi_filtered[['stringId_A', 'stringId_B']].map(lambda x: protein_to_id[x])\n","edge_index = torch.tensor(edges.values.T, dtype=torch.long).to(device)"],"metadata":{"id":"oUA1r1fzco-L","executionInfo":{"status":"ok","timestamp":1741374506197,"user_tz":420,"elapsed":237,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"id":"oUA1r1fzco-L","execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Step 8: Load and preprocess the multi-omics data\n","!wget https://www.webpages.uidaho.edu/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n","file_path = 'mRNA_miRNA_Meth_integrated.csv'\n","df = pd.read_csv(file_path)\n","df.drop(df.columns[0], axis=1, inplace=True)\n","Y = df.iloc[:, -1].copy()\n","\n","# Remove non-numeric columns\n","df = df.select_dtypes(include=[np.number])\n","X = df.values\n","\n","num_classes = len(set(Y))\n","print(\"Number of classes:\", num_classes)\n","num_samples = X.shape[0]\n","print(\"Number of samples:\", num_samples)\n","num_Features = X.shape[1]\n","print(\"Number of Features:\", num_Features)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Encode labels\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-FXwJg3eY2Z","executionInfo":{"status":"ok","timestamp":1741374518714,"user_tz":420,"elapsed":12514,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"197b49fd-b8c6-4419-ba30-8e0563ad11e8"},"id":"s-FXwJg3eY2Z","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-07 19:08:26--  https://www.webpages.uidaho.edu/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n","Resolving www.webpages.uidaho.edu (www.webpages.uidaho.edu)... 129.101.105.230\n","Connecting to www.webpages.uidaho.edu (www.webpages.uidaho.edu)|129.101.105.230|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 123599052 (118M) [application/octet-stream]\n","Saving to: ‘mRNA_miRNA_Meth_integrated.csv’\n","\n","mRNA_miRNA_Meth_int 100%[===================>] 117.87M  24.7MB/s    in 7.0s    \n","\n","2025-03-07 19:08:34 (16.9 MB/s) - ‘mRNA_miRNA_Meth_integrated.csv’ saved [123599052/123599052]\n","\n","Number of classes: 32\n","Number of samples: 8464\n","Number of Features: 2793\n"]}]},{"cell_type":"code","source":["# Step 9: Define the GTN model\n","class GTN(nn.Module):\n","    def __init__(self, num_features, num_classes):\n","        super(GTN, self).__init__()\n","        self.conv1 = TransformerConv(num_features, 128, heads=8)  # Output: 128 * 8 = 1024\n","        self.conv2 = TransformerConv(1024, num_classes, heads=1)  # Input: 1024, Output: num_classes * 1\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","# Step 10: Set up K-fold cross-validation\n","k = 5\n","kf = KFold(n_splits=k, shuffle=True)\n","\n","# Initialize lists to store metrics for each fold\n","precision_scores = []\n","recall_scores = []\n","accuracy_scores = []\n","F1Measure = []\n","\n","# Set hyperparameters\n","hidden_feats = 1024\n","num_layers = 2\n","dropout = 0.5\n","lr = 0.001\n","weight_decay = 0\n","num_epochs = 100"],"metadata":{"id":"mIT3wokBenfm","executionInfo":{"status":"ok","timestamp":1741374518724,"user_tz":420,"elapsed":8,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}}},"id":"mIT3wokBenfm","execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"id":"0b18004c","metadata":{"id":"0b18004c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741374576947,"user_tz":420,"elapsed":58219,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"e89ea80f-b8da-44d9-e441-3bacb016f584"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training time: 0:00:58.167997\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["# Step 11: Training and Evaluation\n","t = now()\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = labels[train_index], labels[test_index]\n","\n","    # Convert labels to tensors within the loop\n","    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n","    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n","\n","    # Calculate the correlation matrix and convert it to an edge index\n","    correlation_matrix_train = np.corrcoef(X_train, rowvar=True)\n","    correlation_matrix_test = np.corrcoef(X_test, rowvar=True)\n","\n","    # Create edge indices based on a correlation threshold\n","    edge_index_train = torch.tensor(np.argwhere((correlation_matrix_train >= 0.9) | (correlation_matrix_train <= -0.9)).T, dtype=torch.long).to(device)\n","    edge_index_test = torch.tensor(np.argwhere((correlation_matrix_test >= 0.9) | (correlation_matrix_test <= -0.9)).T, dtype=torch.long).to(device)\n","\n","    # Prepare training and testing data\n","    train_data = Data(x=torch.tensor(X_train, dtype=torch.float32).to(device), edge_index=edge_index_train, y=y_train)\n","    test_data = Data(x=torch.tensor(X_test, dtype=torch.float32).to(device), edge_index=edge_index_test, y=y_test)\n","\n","    # Initialize the model, criterion, optimizer, and scheduler\n","    model = GTN(X.shape[1], len(np.unique(labels))).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjusted learning rate\n","    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10)\n","\n","    # Training loop\n","    num_epochs = 100\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(train_data)\n","        loss = criterion(out, train_data.y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            out = model(test_data)\n","            pred = out.argmax(dim=1)\n","            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n","            # print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n","\n","    # Testing\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(test_data)\n","        pred = out.argmax(dim=1)\n","        acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n","        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n","        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n","        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n","\n","        accuracy_scores.append(acc)\n","        precision_scores.append(precision)\n","        recall_scores.append(recall)\n","        F1Measure.append(f1)\n","print('Training time: %s' % (now() - t))"]},{"cell_type":"code","source":["# Calculate the average metrics across all folds\n","average_accuracy = np.mean(accuracy_scores)\n","average_precision = np.mean(precision_scores)\n","average_recall = np.mean(recall_scores)\n","average_f1 = np.mean(F1Measure)\n","\n","print(\"Average accuracy =\", average_accuracy)\n","print(\"Accuracy std sev =\", np.std(accuracy_scores))\n","print(\"Average precision =\", average_precision)\n","print(\"Precision std sev =\", np.std(precision_scores))\n","print(\"Average recall =\", average_recall)\n","print(\"Recall std sev =\", np.std(recall_scores))\n","print(\"Average F1 score =\", average_f1)\n","print(\"F1 std dev =\", np.std(F1Measure))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISrYr9jHe80R","executionInfo":{"status":"ok","timestamp":1741374576952,"user_tz":420,"elapsed":4,"user":{"displayName":"Aleksandar Vakanski","userId":"07675307153279708378"}},"outputId":"eae5be4e-74ec-480f-d159-c71082f58c65"},"id":"ISrYr9jHe80R","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy = 0.9417534864041757\n","Accuracy std sev = 0.005470661962832667\n","Average precision = 0.9274269662489297\n","Precision std sev = 0.014940605006746219\n","Average recall = 0.9146926622751235\n","Recall std sev = 0.018925085638013497\n","Average F1 score = 0.9180457610451503\n","F1 std dev = 0.01783253028526419\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}